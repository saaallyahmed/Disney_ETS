{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import datetime as dt\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from keys import (username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews data covers October 2010 through April 2019\n",
    "# On first run of scraping, we discovered that\n",
    "## Crowd Level began to include a prediction starting on August 12, 2014.\n",
    "## Start collecting data in September to grab full month of data\n",
    "dates = []\n",
    "\n",
    "start_date = dt.date(2014, 9, 1)\n",
    "end_date = dt.date(2019, 4, 30)\n",
    "delta = dt.timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    dates.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "    start_date += delta\n",
    "    \n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reworking code - discovered that later years have a prediction before the actual crowd level. \n",
    "\n",
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "browser.visit('https://touringplans.com/login')\n",
    "UserFill = browser.find_by_id('user_session_login')\n",
    "PassFill = browser.find_by_id('user_session_password')\n",
    "UserFill.fill(username)\n",
    "PassFill.fill(password)\n",
    "button = browser.find_by_id('Submit')\n",
    "button.click()\n",
    "\n",
    "# https://touringplans.com/disneyland-resort/crowd-calendar/date/2010-10-13\n",
    "url = \"https://touringplans.com/disneyland-resort/crowd-calendar/date/\"\n",
    "\n",
    "date = []\n",
    "max_temp = []\n",
    "min_temp = []\n",
    "crowd_level = [] \n",
    "\n",
    "for x in dates:\n",
    "    try:\n",
    "        browser.visit(url + x)\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        weathercontainer = soup.find('table', class_=\"weather\")\n",
    "        weather = weathercontainer.find_all('td')\n",
    "        crowds = soup.find_all('span', class_=\"crowd_cal_crowd_level\")\n",
    "        crowdstrong = soup.find_all('strong')\n",
    "\n",
    "        # fill the variables for the current date\n",
    "        # hold temporarily to ensure we get all data points, otherwise whole row will error and drop\n",
    "        temp_max_temp = weather[1].text.split(':')[1].split(' ')[1].split('°')[0]\n",
    "        temp_min_temp = weather[1].text.split(':')[2].replace('\\n\\t', '').replace(' ', '').split('°')[0]\n",
    "        temp_crowd_level = crowdstrong[1].text.strip().split(' ')[0]\n",
    "        print(f\"{x} grabbed\")\n",
    "        # fill the variables for the current date\n",
    "        date.append(x)\n",
    "        max_temp.append(temp_max_temp)\n",
    "        min_temp.append(temp_min_temp)\n",
    "        crowd_level.append(temp_crowd_level)\n",
    "        print(f\"data appended\")\n",
    "#         dt.time.sleep(1)\n",
    "    except (TypeError, NameError, IndexError) as err:\n",
    "        print(f\"{x}: {err}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapedData = {\n",
    "    'date' : date,\n",
    "    'max_temp' : max_temp,\n",
    "    'min_temp' : min_temp,\n",
    "    'crowd_level': crowd_level\n",
    "}\n",
    "scrapedf = pd.DataFrame(scrapedData)\n",
    "scrapedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapedf.to_csv('scraped_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
